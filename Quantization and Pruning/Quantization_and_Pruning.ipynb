{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "# Quantization and Pruning\n",
        "\n",
        "In this notebook, I'll provide code for mobile optimization techniques. These enable reduced model size and latency which makes it ideal for edge and IOT devices. I will start by training a Keras model then compare its model size and accuracy after going through these techniques:\n",
        "\n",
        "* post-training quantization\n",
        "* quantization aware training\n",
        "* weight pruning\n",
        "\n",
        "Let's begin!"
      ],
      "metadata": {
        "id": "AW7DUW4DfpAs"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Imports"
      ],
      "metadata": {
        "id": "i5aJPtuQf6lz"
      }
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "id": "u6poL1m2Hf0O"
      },
      "outputs": [],
      "source": [
        "import tensorflow as tf\n",
        "import numpy as np\n",
        "import os\n",
        "import tempfile\n",
        "import zipfile"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "<a name='utilities'>\n",
        "\n",
        "## Utilities and constants\n",
        "\n",
        "Let's first define a few string constants and utility functions to make our code easier to maintain."
      ],
      "metadata": {
        "id": "kT4DysJrf85Z"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# GLOBAL VARIABLES\n",
        "\n",
        "# String constants for model filenames\n",
        "FILE_WEIGHTS = 'baseline_weights.h5'\n",
        "FILE_NON_QUANTIZED_H5 = 'non_quantized.h5'\n",
        "FILE_NON_QUANTIZED_TFLITE = 'non_quantized.tflite'\n",
        "FILE_PT_QUANTIZED = 'post_training_quantized.tflite'\n",
        "FILE_QAT_QUANTIZED = 'quant_aware_quantized.tflite'\n",
        "FILE_PRUNED_MODEL_H5 = 'pruned_model.h5'\n",
        "FILE_PRUNED_QUANTIZED_TFLITE = 'pruned_quantized.tflite'\n",
        "FILE_PRUNED_NON_QUANTIZED_TFLITE = 'pruned_non_quantized.tflite'\n",
        "\n",
        "# Dictionaries to hold measurements\n",
        "MODEL_SIZE = {}\n",
        "ACCURACY = {}"
      ],
      "metadata": {
        "id": "OjoPXpR1bA8V"
      },
      "execution_count": 3,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# UTILITY FUNCTIONS\n",
        "\n",
        "def print_metric(metric_dict, metric_name):\n",
        "  '''Prints key and values stored in a dictionary'''\n",
        "  for metric, value in metric_dict.items():\n",
        "    print(f'{metric_name} for {metric}: {value}')\n",
        "\n",
        "\n",
        "def model_builder():\n",
        "  '''Returns a shallow CNN for training on the MNIST dataset'''\n",
        "\n",
        "  keras = tf.keras\n",
        "\n",
        "  # Define the model architecture.\n",
        "  model = keras.Sequential([\n",
        "    keras.layers.InputLayer(input_shape=(28, 28)),\n",
        "    keras.layers.Reshape(target_shape=(28, 28, 1)),\n",
        "    keras.layers.Conv2D(filters=12, kernel_size=(3, 3), activation='relu'),\n",
        "    keras.layers.MaxPooling2D(pool_size=(2, 2)),\n",
        "    keras.layers.Flatten(),\n",
        "    keras.layers.Dense(10, activation='softmax')\n",
        "  ])\n",
        "\n",
        "  return model\n",
        "\n",
        "\n",
        "def evaluate_tflite_model(filename, x_test, y_test):\n",
        "  '''\n",
        "  Measures the accuracy of a given TF Lite model and test set\n",
        "\n",
        "  Args:\n",
        "    filename (string) - filename of the model to load\n",
        "    x_test (numpy array) - test images\n",
        "    y_test (numpy array) - test labels\n",
        "\n",
        "  Returns\n",
        "    float showing the accuracy against the test set\n",
        "  '''\n",
        "\n",
        "  # Initialize the TF Lite Interpreter and allocate tensors\n",
        "  interpreter = tf.lite.Interpreter(model_path=filename)\n",
        "  interpreter.allocate_tensors()\n",
        "\n",
        "  # Get input and output index\n",
        "  input_index = interpreter.get_input_details()[0][\"index\"]\n",
        "  output_index = interpreter.get_output_details()[0][\"index\"]\n",
        "\n",
        "  # Initialize empty predictions list\n",
        "  prediction_digits = []\n",
        "\n",
        "  # Run predictions on every image in the \"test\" dataset.\n",
        "  for i, test_image in enumerate(x_test):\n",
        "    # Pre-processing: add batch dimension and convert to float32 to match with\n",
        "    # the model's input data format.\n",
        "    test_image = np.expand_dims(test_image, axis=0).astype(np.float32)\n",
        "    interpreter.set_tensor(input_index, test_image)\n",
        "\n",
        "    # Run inference.\n",
        "    interpreter.invoke()\n",
        "\n",
        "    # Post-processing: remove batch dimension and find the digit with highest\n",
        "    # probability.\n",
        "    output = interpreter.tensor(output_index)\n",
        "    digit = np.argmax(output()[0])\n",
        "    prediction_digits.append(digit)\n",
        "\n",
        "  # Compare prediction results with ground truth labels to calculate accuracy.\n",
        "  prediction_digits = np.array(prediction_digits)\n",
        "  accuracy = (prediction_digits == y_test).mean()\n",
        "\n",
        "  return accuracy\n",
        "\n",
        "\n",
        "def get_gzipped_model_size(file):\n",
        "  '''Returns size of gzipped model, in bytes.'''\n",
        "  _, zipped_file = tempfile.mkstemp('.zip')\n",
        "  with zipfile.ZipFile(zipped_file, 'w', compression=zipfile.ZIP_DEFLATED) as f:\n",
        "    f.write(file)\n",
        "\n",
        "  return os.path.getsize(zipped_file)"
      ],
      "metadata": {
        "id": "A4LdV-mRclxO"
      },
      "execution_count": 4,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Download and Prepare the Dataset"
      ],
      "metadata": {
        "id": "pqM8l5rAgEfb"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "I will be using the [MNIST](https://keras.io/api/datasets/mnist/) dataset which is hosted in [Keras Datasets](https://keras.io/api/datasets/). Some of the helper files in this notebook are made to work with this dataset so if you decide to switch to a different dataset, make sure to check if those helper functions need to be modified (e.g. shape of the Flatten layer in your model)."
      ],
      "metadata": {
        "id": "jSnOn2icgH44"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Load MNIST dataset\n",
        "mnist = tf.keras.datasets.mnist\n",
        "(train_images, train_labels), (test_images, test_labels) = mnist.load_data()\n",
        "\n",
        "# Normalize the input image so that each pixel value is between 0 to 1.\n",
        "train_images = train_images / 255.0\n",
        "test_images = test_images / 255.0"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "1_mNgNeQgCo5",
        "outputId": "30eb3f7a-6134-4388-d82c-a725230a86c0"
      },
      "execution_count": 5,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Downloading data from https://storage.googleapis.com/tensorflow/tf-keras-datasets/mnist.npz\n",
            "11490434/11490434 [==============================] - 1s 0us/step\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Baseline Model\n",
        "\n",
        "I will first build and train a Keras model. This will be the baseline where I will be comparing the mobile optimized versions later on. This will just be a shallow CNN with a softmax output to classify a given MNIST digit. You can review the `model_builder()` function in the utilities at the top of this notebook but I also printed the model summary below to show the architecture.\n",
        "\n",
        "I will also save the weights so I can reinitialize the other models later the same way. This is not needed in real projects but for this notebook, it would be good to have the same initial state later so I can compare the effects of the optimizations."
      ],
      "metadata": {
        "id": "-mooBXOqgOk3"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Create the baseline model\n",
        "baseline_model = model_builder()\n",
        "\n",
        "# Save the initial weights for use later\n",
        "baseline_model.save_weights(FILE_WEIGHTS)\n",
        "\n",
        "# Print the model summary\n",
        "baseline_model.summary()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "iEHC2An7gMrH",
        "outputId": "4de0b290-d1eb-404e-c48b-24d4e99e61fe"
      },
      "execution_count": 6,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Model: \"sequential\"\n",
            "_________________________________________________________________\n",
            " Layer (type)                Output Shape              Param #   \n",
            "=================================================================\n",
            " reshape (Reshape)           (None, 28, 28, 1)         0         \n",
            "                                                                 \n",
            " conv2d (Conv2D)             (None, 26, 26, 12)        120       \n",
            "                                                                 \n",
            " max_pooling2d (MaxPooling2  (None, 13, 13, 12)        0         \n",
            " D)                                                              \n",
            "                                                                 \n",
            " flatten (Flatten)           (None, 2028)              0         \n",
            "                                                                 \n",
            " dense (Dense)               (None, 10)                20290     \n",
            "                                                                 \n",
            "=================================================================\n",
            "Total params: 20410 (79.73 KB)\n",
            "Trainable params: 20410 (79.73 KB)\n",
            "Non-trainable params: 0 (0.00 Byte)\n",
            "_________________________________________________________________\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Setup the model for training\n",
        "baseline_model.compile(optimizer='adam',\n",
        "              loss='sparse_categorical_crossentropy',\n",
        "              metrics=['accuracy'])\n",
        "\n",
        "# Train the model\n",
        "baseline_model.fit(train_images, train_labels, epochs=1, shuffle=False)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "SbIymlR8ghzT",
        "outputId": "1aa2602f-5346-4755-9103-4901898ba2fd"
      },
      "execution_count": 7,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "1875/1875 [==============================] - 25s 13ms/step - loss: 0.2652 - accuracy: 0.9263\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<keras.src.callbacks.History at 0x79b7154112a0>"
            ]
          },
          "metadata": {},
          "execution_count": 7
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Get the baseline accuracy\n",
        "_, ACCURACY['baseline Keras model'] = baseline_model.evaluate(test_images, test_labels)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "MtbHG2M-gjuz",
        "outputId": "1dc46cce-53fc-49a0-f106-1eeb81781f49"
      },
      "execution_count": 8,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "313/313 [==============================] - 3s 9ms/step - loss: 0.1233 - accuracy: 0.9634\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Next, I will save the Keras model as a file and record its size as well."
      ],
      "metadata": {
        "id": "p9ht3R5AgnaR"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Save the Keras model\n",
        "baseline_model.save(FILE_NON_QUANTIZED_H5, include_optimizer=False)\n",
        "\n",
        "# Save and get the model size\n",
        "MODEL_SIZE['baseline h5'] = os.path.getsize(FILE_NON_QUANTIZED_H5)\n",
        "\n",
        "# Print records so far\n",
        "print_metric(ACCURACY, \"test accuracy\")\n",
        "print_metric(MODEL_SIZE, \"model size in bytes\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "5iXwnoPhglNW",
        "outputId": "b33845a3-e5c6-477b-9bc7-b2ce76cd95ce"
      },
      "execution_count": 9,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "test accuracy for baseline Keras model: 0.9634000062942505\n",
            "model size in bytes for baseline h5: 98968\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/keras/src/engine/training.py:3000: UserWarning: You are saving your model as an HDF5 file via `model.save()`. This file format is considered legacy. We recommend using instead the native Keras format, e.g. `model.save('my_model.keras')`.\n",
            "  saving_api.save_model(\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Convert the model to TF Lite format"
      ],
      "metadata": {
        "id": "bNGwa1HAgsr0"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def convert_tflite(model, filename, quantize=False):\n",
        "  '''\n",
        "  Converts the model to TF Lite format and writes to a file\n",
        "\n",
        "  Args:\n",
        "    model (Keras model) - model to convert to TF Lite\n",
        "    filename (string) - string to use when saving the file\n",
        "    quantize (bool) - flag to indicate quantization\n",
        "\n",
        "  Returns:\n",
        "    None\n",
        "  '''\n",
        "\n",
        "  # Initialize the converter\n",
        "  converter = tf.lite.TFLiteConverter.from_keras_model(model)\n",
        "\n",
        "  # Set for quantization if flag is set to True\n",
        "  if quantize:\n",
        "    converter.optimizations = [tf.lite.Optimize.DEFAULT]\n",
        "\n",
        "  # Convert the model\n",
        "  tflite_model = converter.convert()\n",
        "\n",
        "  # Save the model.\n",
        "  with open(filename, 'wb') as f:\n",
        "    f.write(tflite_model)"
      ],
      "metadata": {
        "id": "ResIS7-SgqGW"
      },
      "execution_count": 10,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "mIE2CadBgvmz"
      },
      "source": [
        "# Convert baseline model\n",
        "convert_tflite(baseline_model, FILE_NON_QUANTIZED_TFLITE)"
      ],
      "execution_count": 11,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "5H61feiOZkcI",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "c39eb157-b9bb-46bf-abc6-46f012ad9cf9"
      },
      "source": [
        "MODEL_SIZE['non quantized tflite'] = os.path.getsize(FILE_NON_QUANTIZED_TFLITE)\n",
        "\n",
        "print_metric(MODEL_SIZE, 'model size in bytes')"
      ],
      "execution_count": 12,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "model size in bytes for baseline h5: 98968\n",
            "model size in bytes for non quantized tflite: 85012\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "ACCURACY['non quantized tflite'] = evaluate_tflite_model(FILE_NON_QUANTIZED_TFLITE, test_images, test_labels)"
      ],
      "metadata": {
        "id": "gqMvr10ug1HG"
      },
      "execution_count": 13,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "print_metric(ACCURACY, 'test accuracy')"
      ],
      "metadata": {
        "id": "5ZUunCoEg3Ob"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Post-Training Quantization\n",
        "\n",
        "Now that I have the baseline metrics, I can now observe the effects of quantization. This process involves converting floating point representations into integer to reduce model size and achieve faster computation.\n",
        "\n",
        "As shown in the `convert_tflite()` helper function earlier, I can easily do [post-training quantization](https://www.tensorflow.org/lite/performance/post_training_quantization) with the TF Lite API. I just need to set the converter optimization and assign an [Optimize](https://www.tensorflow.org/api_docs/python/tf/lite/Optimize) Enum.\n",
        "\n",
        "I will set the `quantize` flag to do that and get the metrics again."
      ],
      "metadata": {
        "id": "4qTxFxoSg_pm"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Convert and quantize the baseline model\n",
        "convert_tflite(baseline_model, FILE_PT_QUANTIZED, quantize=True)"
      ],
      "metadata": {
        "id": "FdzTpChKg7zo"
      },
      "execution_count": 14,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Get the model size\n",
        "MODEL_SIZE['post training quantized tflite'] = os.path.getsize(FILE_PT_QUANTIZED)\n",
        "\n",
        "print_metric(MODEL_SIZE, 'model size')"
      ],
      "metadata": {
        "id": "Gy_mfw3JhKlJ",
        "outputId": "614580d9-8a35-4330-8a0c-9960def91d77",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": 15,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "model size for baseline h5: 98968\n",
            "model size for non quantized tflite: 85012\n",
            "model size for post training quantized tflite: 24256\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "ACCURACY['post training quantized tflite'] = evaluate_tflite_model(FILE_PT_QUANTIZED, test_images, test_labels)"
      ],
      "metadata": {
        "id": "l02eOFRJhLqr"
      },
      "execution_count": 16,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "print_metric(ACCURACY, 'test accuracy')"
      ],
      "metadata": {
        "id": "W37tB5uUhNBc",
        "outputId": "aa5a581f-5ecc-4226-fe8d-e30e922a7acf",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": 17,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "test accuracy for baseline Keras model: 0.9634000062942505\n",
            "test accuracy for non quantized tflite: 0.9634\n",
            "test accuracy for post training quantized tflite: 0.9639\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Quantization Aware Training"
      ],
      "metadata": {
        "id": "mv_r4KpYhQkF"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "When post-training quantization results in loss of accuracy that is unacceptable for your application, you can consider doing [quantization aware training](https://www.tensorflow.org/model_optimization/guide/quantization/training) before quantizing the model. This simulates the loss of precision by inserting fake quant nodes in the model during training. That way, your model will learn to adapt with the loss of precision to get more accurate predictions.\n",
        "\n",
        "The [Tensorflow Model Optimization Toolkit](https://www.tensorflow.org/model_optimization) provides a [quantize_model()](https://www.tensorflow.org/model_optimization/api_docs/python/tfmot/quantization/keras/quantize_model) method to do this quickly and you will see that below. But first, let's install the toolkit into the notebook environment."
      ],
      "metadata": {
        "id": "g1FqkvxzhSA_"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Install the toolkit\n",
        "!pip install tensorflow_model_optimization"
      ],
      "metadata": {
        "id": "C5ZqnefQhOBf",
        "outputId": "2b314808-e78e-4221-c132-e8f49969e08f",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": 18,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Collecting tensorflow_model_optimization\n",
            "  Downloading tensorflow_model_optimization-0.7.5-py2.py3-none-any.whl (241 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m241.2/241.2 kB\u001b[0m \u001b[31m3.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: absl-py~=1.2 in /usr/local/lib/python3.10/dist-packages (from tensorflow_model_optimization) (1.4.0)\n",
            "Requirement already satisfied: dm-tree~=0.1.1 in /usr/local/lib/python3.10/dist-packages (from tensorflow_model_optimization) (0.1.8)\n",
            "Requirement already satisfied: numpy~=1.23 in /usr/local/lib/python3.10/dist-packages (from tensorflow_model_optimization) (1.23.5)\n",
            "Requirement already satisfied: six~=1.14 in /usr/local/lib/python3.10/dist-packages (from tensorflow_model_optimization) (1.16.0)\n",
            "Installing collected packages: tensorflow_model_optimization\n",
            "Successfully installed tensorflow_model_optimization-0.7.5\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import tensorflow_model_optimization as tfmot\n",
        "\n",
        "# method to quantize a Keras model\n",
        "quantize_model = tfmot.quantization.keras.quantize_model\n",
        "\n",
        "# Define the model architecture.\n",
        "model_to_quantize = model_builder()\n",
        "\n",
        "# Reinitialize weights with saved file\n",
        "model_to_quantize.load_weights(FILE_WEIGHTS)\n",
        "\n",
        "# Quantize the model\n",
        "q_aware_model = quantize_model(model_to_quantize)\n",
        "\n",
        "# `quantize_model` requires a recompile.\n",
        "q_aware_model.compile(optimizer='adam',\n",
        "              loss='sparse_categorical_crossentropy',\n",
        "              metrics=['accuracy'])\n",
        "\n",
        "q_aware_model.summary()"
      ],
      "metadata": {
        "id": "GJFCXA_IhWXl",
        "outputId": "0aa030d9-76d4-47ba-ce51-a1e1cd4afa4c",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": 19,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Model: \"sequential_1\"\n",
            "_________________________________________________________________\n",
            " Layer (type)                Output Shape              Param #   \n",
            "=================================================================\n",
            " quantize_layer (QuantizeLa  (None, 28, 28)            3         \n",
            " yer)                                                            \n",
            "                                                                 \n",
            " quant_reshape_1 (QuantizeW  (None, 28, 28, 1)         1         \n",
            " rapperV2)                                                       \n",
            "                                                                 \n",
            " quant_conv2d_1 (QuantizeWr  (None, 26, 26, 12)        147       \n",
            " apperV2)                                                        \n",
            "                                                                 \n",
            " quant_max_pooling2d_1 (Qua  (None, 13, 13, 12)        1         \n",
            " ntizeWrapperV2)                                                 \n",
            "                                                                 \n",
            " quant_flatten_1 (QuantizeW  (None, 2028)              1         \n",
            " rapperV2)                                                       \n",
            "                                                                 \n",
            " quant_dense_1 (QuantizeWra  (None, 10)                20295     \n",
            " pperV2)                                                         \n",
            "                                                                 \n",
            "=================================================================\n",
            "Total params: 20448 (79.88 KB)\n",
            "Trainable params: 20410 (79.73 KB)\n",
            "Non-trainable params: 38 (152.00 Byte)\n",
            "_________________________________________________________________\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Train the model\n",
        "q_aware_model.fit(train_images, train_labels, epochs=1, shuffle=False)"
      ],
      "metadata": {
        "id": "lFujTxSshX94",
        "outputId": "21736ccf-c264-489d-9cd2-856d58c3d213",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": 20,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "1875/1875 [==============================] - 39s 20ms/step - loss: 0.2694 - accuracy: 0.9248\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<keras.src.callbacks.History at 0x79b71599d600>"
            ]
          },
          "metadata": {},
          "execution_count": 20
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Reinitialize the dictionary\n",
        "ACCURACY = {}\n",
        "\n",
        "# Get the accuracy of the quantization aware trained model (not yet quantized)\n",
        "_, ACCURACY['quantization aware non-quantized'] = q_aware_model.evaluate(test_images, test_labels, verbose=0)\n",
        "print_metric(ACCURACY, 'test accuracy')"
      ],
      "metadata": {
        "id": "YOGN0zVAhaVD",
        "outputId": "0572701b-2749-4167-e0cb-e6dfc05daf91",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": 21,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "test accuracy for quantization aware non-quantized: 0.9635999798774719\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Convert and quantize the model.\n",
        "convert_tflite(q_aware_model, FILE_QAT_QUANTIZED, quantize=True)\n",
        "\n",
        "# Get the accuracy of the quantized model\n",
        "ACCURACY['quantization aware quantized'] = evaluate_tflite_model(FILE_QAT_QUANTIZED, test_images, test_labels)\n",
        "print_metric(ACCURACY, 'test accuracy')"
      ],
      "metadata": {
        "id": "LdeAba1rhbb5",
        "outputId": "e464ebcb-cf29-4453-eea5-487452771841",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": 24,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/tensorflow/lite/python/convert.py:887: UserWarning: Statistics for quantized inputs were expected, but not specified; continuing anyway.\n",
            "  warnings.warn(\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "test accuracy for quantization aware non-quantized: 0.9635999798774719\n",
            "test accuracy for quantization aware quantized: 0.9636\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Pruning\n",
        "\n",
        "Let's now move on to another technique for reducing model size: [Pruning](https://www.tensorflow.org/model_optimization/guide/pruning/pruning_with_keras). This process involves zeroing out insignificant (i.e. low magnitude) weights. The intuition is these weights do not contribute as much to making predictions so we can remove them and get the same result. Making the weights sparse helps in compressing the model more efficiently and you will see that in this section."
      ],
      "metadata": {
        "id": "BRd7ESGehi7Q"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "The Tensorflow Model Optimization Toolkit again has a convenience method for this. The [prune_low_magnitude()](https://www.tensorflow.org/model_optimization/api_docs/python/tfmot/sparsity/keras/prune_low_magnitude) method puts wrappers in a Keras model so it can be pruned during training. I will pass in the baseline model that I already trained earlier. You will notice that the model summary show increased params because of the wrapper layers added by the pruning method.\n",
        "\n",
        "We can set how the pruning is done during training. Below, I will use [PolynomialDecay](https://www.tensorflow.org/model_optimization/api_docs/python/tfmot/sparsity/keras/PolynomialDecay) to indicate how the sparsity ramps up with each step. Another option available in the library is [Constant Sparsity](https://www.tensorflow.org/model_optimization/api_docs/python/tfmot/sparsity/keras/ConstantSparsity)."
      ],
      "metadata": {
        "id": "4uJA4YAHiCLq"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Get the pruning method\n",
        "prune_low_magnitude = tfmot.sparsity.keras.prune_low_magnitude\n",
        "\n",
        "# Compute end step to finish pruning after 2 epochs.\n",
        "batch_size = 128\n",
        "epochs = 2\n",
        "validation_split = 0.1 # 10% of training set will be used for validation set.\n",
        "\n",
        "num_images = train_images.shape[0] * (1 - validation_split)\n",
        "end_step = np.ceil(num_images / batch_size).astype(np.int32) * epochs\n",
        "\n",
        "# Define pruning schedule.\n",
        "pruning_params = {\n",
        "      'pruning_schedule': tfmot.sparsity.keras.PolynomialDecay(initial_sparsity=0.50,\n",
        "                                                               final_sparsity=0.80,\n",
        "                                                               begin_step=0,\n",
        "                                                               end_step=end_step)\n",
        "}\n",
        "\n",
        "# Pass in the trained baseline model\n",
        "model_for_pruning = prune_low_magnitude(baseline_model, **pruning_params)\n",
        "\n",
        "# `prune_low_magnitude` requires a recompile.\n",
        "model_for_pruning.compile(optimizer='adam',\n",
        "              loss='sparse_categorical_crossentropy',\n",
        "              metrics=['accuracy'])\n",
        "\n",
        "model_for_pruning.summary()"
      ],
      "metadata": {
        "id": "KjKaCuo3hc1V",
        "outputId": "9158b96a-5296-4398-d9af-b35e4c68214f",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": 25,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Model: \"sequential\"\n",
            "_________________________________________________________________\n",
            " Layer (type)                Output Shape              Param #   \n",
            "=================================================================\n",
            " prune_low_magnitude_reshap  (None, 28, 28, 1)         1         \n",
            " e (PruneLowMagnitude)                                           \n",
            "                                                                 \n",
            " prune_low_magnitude_conv2d  (None, 26, 26, 12)        230       \n",
            "  (PruneLowMagnitude)                                            \n",
            "                                                                 \n",
            " prune_low_magnitude_max_po  (None, 13, 13, 12)        1         \n",
            " oling2d (PruneLowMagnitude                                      \n",
            " )                                                               \n",
            "                                                                 \n",
            " prune_low_magnitude_flatte  (None, 2028)              1         \n",
            " n (PruneLowMagnitude)                                           \n",
            "                                                                 \n",
            " prune_low_magnitude_dense   (None, 10)                40572     \n",
            " (PruneLowMagnitude)                                             \n",
            "                                                                 \n",
            "=================================================================\n",
            "Total params: 40805 (159.41 KB)\n",
            "Trainable params: 20410 (79.73 KB)\n",
            "Non-trainable params: 20395 (79.69 KB)\n",
            "_________________________________________________________________\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Preview model weights\n",
        "model_for_pruning.weights[1]"
      ],
      "metadata": {
        "id": "TVq_i96WhkSU",
        "outputId": "06d21adc-002f-4399-b51b-572f9168cd35",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": 26,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<tf.Variable 'conv2d/kernel:0' shape=(3, 3, 1, 12) dtype=float32, numpy=\n",
              "array([[[[ 0.32880077, -0.00370286, -0.43661624, -0.7047039 ,\n",
              "           0.25139543, -0.03380103,  0.12268101, -0.47647282,\n",
              "           0.06151805,  0.19884391, -0.09929218,  0.64335364]],\n",
              "\n",
              "        [[ 0.44360656,  0.1415229 ,  0.05076738, -0.22037843,\n",
              "           0.33123186,  0.28065774,  0.16067812, -0.4581575 ,\n",
              "           0.10179768,  0.41382894,  0.06424277, -0.54322696]],\n",
              "\n",
              "        [[ 0.2417517 , -0.15722294,  0.38414168,  0.39335033,\n",
              "          -0.06849516,  0.4636034 , -0.13752379, -0.4403433 ,\n",
              "           0.22671011,  0.38459358,  0.04796667, -0.33809617]]],\n",
              "\n",
              "\n",
              "       [[[-0.1376045 ,  0.07523245, -0.3559213 , -0.6807172 ,\n",
              "           0.09241283, -0.15107277, -0.13837588, -0.01652741,\n",
              "           0.2560323 ,  0.35705173,  0.28778088, -0.59544927]],\n",
              "\n",
              "        [[ 0.3113292 ,  0.28261712,  0.29757544,  0.2872469 ,\n",
              "           0.31608188,  0.17711316,  0.10828544,  0.19653368,\n",
              "          -0.02341902,  0.2382372 ,  0.11239841, -0.52644646]],\n",
              "\n",
              "        [[ 0.03138264, -0.0309    , -0.01040568,  0.3292966 ,\n",
              "           0.15024501,  0.49715513,  0.36012396,  0.23403156,\n",
              "           0.08955175, -0.16831994,  0.18270017,  0.3607909 ]]],\n",
              "\n",
              "\n",
              "       [[[-0.6821093 ,  0.12011209, -0.18743801, -0.6026334 ,\n",
              "           0.0408062 , -0.7926231 , -0.128294  ,  0.4952545 ,\n",
              "           0.10969192, -0.8308588 ,  0.24512693, -0.3716593 ]],\n",
              "\n",
              "        [[-0.59186375,  0.11128492,  0.3172175 ,  0.03270301,\n",
              "           0.23145579, -0.6200238 ,  0.21962787,  0.2118481 ,\n",
              "           0.24578552, -0.7474479 ,  0.14588316,  0.45435408]],\n",
              "\n",
              "        [[-0.07349839,  0.18572941,  0.18940857,  0.5109458 ,\n",
              "           0.2727815 , -0.08970218,  0.01281634,  0.15569758,\n",
              "          -0.14187051, -0.5029927 ,  0.12975635,  0.3055937 ]]]],\n",
              "      dtype=float32)>"
            ]
          },
          "metadata": {},
          "execution_count": 26
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Callback to update pruning wrappers at each step\n",
        "callbacks = [\n",
        "  tfmot.sparsity.keras.UpdatePruningStep(),\n",
        "]\n",
        "\n",
        "# Train and prune the model\n",
        "model_for_pruning.fit(train_images, train_labels,\n",
        "                  epochs=epochs, validation_split=validation_split,\n",
        "                  callbacks=callbacks)"
      ],
      "metadata": {
        "id": "4GYFtI6Ahl7g",
        "outputId": "6cd1cbf9-bfdb-4807-e69b-394dc12ee5d8",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": 27,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 1/2\n",
            "1688/1688 [==============================] - 24s 13ms/step - loss: 0.1571 - accuracy: 0.9604 - val_loss: 0.0949 - val_accuracy: 0.9743\n",
            "Epoch 2/2\n",
            "1688/1688 [==============================] - 21s 12ms/step - loss: 0.1088 - accuracy: 0.9694 - val_loss: 0.0809 - val_accuracy: 0.9782\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<keras.src.callbacks.History at 0x79b6f80cbd00>"
            ]
          },
          "metadata": {},
          "execution_count": 27
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Preview model weights\n",
        "model_for_pruning.weights[1]"
      ],
      "metadata": {
        "id": "AN0OvGpghnYM",
        "outputId": "fd9f5fd1-27c7-42b2-991f-9d2393f08f04",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": 28,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<tf.Variable 'conv2d/kernel:0' shape=(3, 3, 1, 12) dtype=float32, numpy=\n",
              "array([[[[-0.        , -0.        , -0.        , -0.7656396 ,\n",
              "           0.        , -0.        , -0.        , -1.0429403 ,\n",
              "          -0.        , -0.        , -0.        ,  0.8965841 ]],\n",
              "\n",
              "        [[ 0.88872886, -0.        ,  0.        ,  0.        ,\n",
              "           0.        , -0.        , -0.        , -0.        ,\n",
              "           0.        ,  1.1694382 , -0.        , -0.8108421 ]],\n",
              "\n",
              "        [[-0.        , -0.        ,  0.85616434,  0.        ,\n",
              "           0.        ,  0.8353219 ,  0.        , -0.        ,\n",
              "           0.        , -0.        , -0.        , -0.        ]]],\n",
              "\n",
              "\n",
              "       [[[-0.        , -0.        ,  0.        , -0.68185574,\n",
              "           0.        , -0.        , -0.        , -0.        ,\n",
              "          -0.        , -0.        , -0.        , -1.3538901 ]],\n",
              "\n",
              "        [[-0.        , -0.        ,  0.        ,  0.        ,\n",
              "           0.        , -0.        , -0.        , -0.        ,\n",
              "          -0.        , -0.        , -0.        , -0.        ]],\n",
              "\n",
              "        [[-0.        , -0.        ,  0.        ,  0.        ,\n",
              "           0.        ,  0.7980285 ,  1.000684  , -0.        ,\n",
              "           0.        , -0.        , -0.        , -0.        ]]],\n",
              "\n",
              "\n",
              "       [[[-0.9586747 , -0.        ,  0.        , -1.1423001 ,\n",
              "           0.        , -1.1301464 , -0.        ,  1.0486004 ,\n",
              "          -0.        , -1.0613761 , -0.        , -0.        ]],\n",
              "\n",
              "        [[-0.        , -0.        ,  0.        ,  0.        ,\n",
              "           0.        , -1.1598732 , -0.        , -0.        ,\n",
              "          -0.        , -0.94478893, -0.        ,  0.75036997]],\n",
              "\n",
              "        [[-0.        , -0.        ,  0.        ,  1.0766276 ,\n",
              "           0.        , -0.        ,  0.        , -0.        ,\n",
              "           0.        , -1.0689092 ,  0.        , -0.        ]]]],\n",
              "      dtype=float32)>"
            ]
          },
          "metadata": {},
          "execution_count": 28
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "After pruning, we can remove the wrapper layers to have the same layers and params as the baseline model."
      ],
      "metadata": {
        "id": "ELJUukZviPo8"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Remove pruning wrappers\n",
        "model_for_export = tfmot.sparsity.keras.strip_pruning(model_for_pruning)\n",
        "model_for_export.summary()"
      ],
      "metadata": {
        "id": "K1xHSwlVh9va",
        "outputId": "db883574-7f9c-45ac-cb87-8c78ed1dee09",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": 29,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Model: \"sequential\"\n",
            "_________________________________________________________________\n",
            " Layer (type)                Output Shape              Param #   \n",
            "=================================================================\n",
            " reshape (Reshape)           (None, 28, 28, 1)         0         \n",
            "                                                                 \n",
            " conv2d (Conv2D)             (None, 26, 26, 12)        120       \n",
            "                                                                 \n",
            " max_pooling2d (MaxPooling2  (None, 13, 13, 12)        0         \n",
            " D)                                                              \n",
            "                                                                 \n",
            " flatten (Flatten)           (None, 2028)              0         \n",
            "                                                                 \n",
            " dense (Dense)               (None, 10)                20290     \n",
            "                                                                 \n",
            "=================================================================\n",
            "Total params: 20410 (79.73 KB)\n",
            "Trainable params: 20410 (79.73 KB)\n",
            "Non-trainable params: 0 (0.00 Byte)\n",
            "_________________________________________________________________\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Preview model weights (index 1 earlier is now 0 because pruning wrappers were removed)\n",
        "model_for_export.weights[0]"
      ],
      "metadata": {
        "id": "khbCPM1JiTbs",
        "outputId": "6ba2d57e-6a45-45c4-c8ab-da225df9fcb2",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": 30,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<tf.Variable 'conv2d/kernel:0' shape=(3, 3, 1, 12) dtype=float32, numpy=\n",
              "array([[[[-0.        , -0.        , -0.        , -0.7656396 ,\n",
              "           0.        , -0.        , -0.        , -1.0429403 ,\n",
              "          -0.        , -0.        , -0.        ,  0.8965841 ]],\n",
              "\n",
              "        [[ 0.88872886, -0.        ,  0.        ,  0.        ,\n",
              "           0.        , -0.        , -0.        , -0.        ,\n",
              "           0.        ,  1.1694382 , -0.        , -0.8108421 ]],\n",
              "\n",
              "        [[-0.        , -0.        ,  0.85616434,  0.        ,\n",
              "           0.        ,  0.8353219 ,  0.        , -0.        ,\n",
              "           0.        , -0.        , -0.        , -0.        ]]],\n",
              "\n",
              "\n",
              "       [[[-0.        , -0.        ,  0.        , -0.68185574,\n",
              "           0.        , -0.        , -0.        , -0.        ,\n",
              "          -0.        , -0.        , -0.        , -1.3538901 ]],\n",
              "\n",
              "        [[-0.        , -0.        ,  0.        ,  0.        ,\n",
              "           0.        , -0.        , -0.        , -0.        ,\n",
              "          -0.        , -0.        , -0.        , -0.        ]],\n",
              "\n",
              "        [[-0.        , -0.        ,  0.        ,  0.        ,\n",
              "           0.        ,  0.7980285 ,  1.000684  , -0.        ,\n",
              "           0.        , -0.        , -0.        , -0.        ]]],\n",
              "\n",
              "\n",
              "       [[[-0.9586747 , -0.        ,  0.        , -1.1423001 ,\n",
              "           0.        , -1.1301464 , -0.        ,  1.0486004 ,\n",
              "          -0.        , -1.0613761 , -0.        , -0.        ]],\n",
              "\n",
              "        [[-0.        , -0.        ,  0.        ,  0.        ,\n",
              "           0.        , -1.1598732 , -0.        , -0.        ,\n",
              "          -0.        , -0.94478893, -0.        ,  0.75036997]],\n",
              "\n",
              "        [[-0.        , -0.        ,  0.        ,  1.0766276 ,\n",
              "           0.        , -0.        ,  0.        , -0.        ,\n",
              "           0.        , -1.0689092 ,  0.        , -0.        ]]]],\n",
              "      dtype=float32)>"
            ]
          },
          "metadata": {},
          "execution_count": 30
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "You will notice below that the pruned model will have the same file size as the baseline_model when saved as H5. This is to be expected. The improvement will be noticeable when you compress the model as will be shown in the cell after this."
      ],
      "metadata": {
        "id": "fwXyNpp2ibdl"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Save Keras model\n",
        "model_for_export.save(FILE_PRUNED_MODEL_H5, include_optimizer=False)\n",
        "\n",
        "# Get uncompressed model size of baseline and pruned models\n",
        "MODEL_SIZE = {}\n",
        "MODEL_SIZE['baseline h5'] = os.path.getsize(FILE_NON_QUANTIZED_H5)\n",
        "MODEL_SIZE['pruned non quantized h5'] = os.path.getsize(FILE_PRUNED_MODEL_H5)\n",
        "\n",
        "print_metric(MODEL_SIZE, 'model_size in bytes')"
      ],
      "metadata": {
        "id": "lUslrBFciU_A",
        "outputId": "22ca191d-d2c9-4747-9d5c-79a4abbfaab1",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": 31,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/keras/src/engine/training.py:3000: UserWarning: You are saving your model as an HDF5 file via `model.save()`. This file format is considered legacy. We recommend using instead the native Keras format, e.g. `model.save('my_model.keras')`.\n",
            "  saving_api.save_model(\n",
            "WARNING:tensorflow:Compiled the loaded model, but the compiled metrics have yet to be built. `model.compile_metrics` will be empty until you train or evaluate the model.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "model_size in bytes for baseline h5: 98968\n",
            "model_size in bytes for pruned non quantized h5: 98968\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Get compressed size of baseline and pruned models\n",
        "MODEL_SIZE = {}\n",
        "MODEL_SIZE['baseline h5'] = get_gzipped_model_size(FILE_NON_QUANTIZED_H5)\n",
        "MODEL_SIZE['pruned non quantized h5'] = get_gzipped_model_size(FILE_PRUNED_MODEL_H5)\n",
        "\n",
        "print_metric(MODEL_SIZE, \"gzipped model size in bytes\")"
      ],
      "metadata": {
        "id": "Wa0gSRIDiYWB",
        "outputId": "fd5a3ba8-516a-4e79-b51b-ecb8e6b12224",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": 32,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "gzipped model size in bytes for baseline h5: 78077\n",
            "gzipped model size in bytes for pruned non quantized h5: 25831\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Convert and quantize the pruned model.\n",
        "pruned_quantized_tflite = convert_tflite(model_for_export, FILE_PRUNED_QUANTIZED_TFLITE, quantize=True)\n",
        "\n",
        "# Compress and get the model size\n",
        "MODEL_SIZE['pruned quantized tflite'] = get_gzipped_model_size(FILE_PRUNED_QUANTIZED_TFLITE)\n",
        "print_metric(MODEL_SIZE, \"gzipped model size in bytes\")"
      ],
      "metadata": {
        "id": "rL1B40gxieLN",
        "outputId": "fbf97b89-27f8-4e53-9b12-dbfdd1d41a75",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": 33,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "gzipped model size in bytes for baseline h5: 78077\n",
            "gzipped model size in bytes for pruned non quantized h5: 25831\n",
            "gzipped model size in bytes for pruned quantized tflite: 8593\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Get accuracy of pruned Keras and TF Lite models\n",
        "ACCURACY = {}\n",
        "\n",
        "_, ACCURACY['pruned model h5'] = model_for_pruning.evaluate(test_images, test_labels)\n",
        "ACCURACY['pruned and quantized tflite'] = evaluate_tflite_model(FILE_PRUNED_QUANTIZED_TFLITE, test_images, test_labels)\n",
        "\n",
        "print_metric(ACCURACY, 'accuracy')"
      ],
      "metadata": {
        "id": "eH7aWAVmifxp",
        "outputId": "ffb24dfb-1e6f-4fca-f0f3-aaba9c25017f",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": 34,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "313/313 [==============================] - 2s 7ms/step - loss: 0.0988 - accuracy: 0.9711\n",
            "accuracy for pruned model h5: 0.9710999727249146\n",
            "accuracy for pruned and quantized tflite: 0.9713\n"
          ]
        }
      ]
    }
  ]
}