{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "d0c4ee4a",
   "metadata": {},
   "source": [
    "## Workflow:\n",
    "- Import Dependencies\n",
    "- Download Dataset\n",
    "- Read file\n",
    "- Process Text\n",
    "- Creating training and target examples\n",
    "- Creating training batches\n",
    "- Build model\n",
    "- Trying model\n",
    "- Creating Checkpoints\n",
    "- Executing Training\n",
    "- Export Generator"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3898abd3",
   "metadata": {},
   "source": [
    "### Import Dependencies"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "e87c3585",
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "\n",
    "import numpy as np\n",
    "import os\n",
    "import time"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "54e5573f",
   "metadata": {},
   "source": [
    "### Download Shakespeare Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "329a625a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Downloading data from https://storage.googleapis.com/download.tensorflow.org/data/shakespeare.txt\n",
      "1115394/1115394 [==============================] - 2s 2us/step\n"
     ]
    }
   ],
   "source": [
    "path_to_file = tf.keras.utils.get_file('shakespeare.txt', 'https://storage.googleapis.com/download.tensorflow.org/data/shakespeare.txt')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "62c2fe02",
   "metadata": {},
   "source": [
    "### Read the File"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "1e482322",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Length of text: 1115394 characters\n"
     ]
    }
   ],
   "source": [
    "# Read, then decode for py2 compat.\n",
    "text = open(path_to_file, 'rb').read().decode('utf-8')  # a string\n",
    "# length of text is the number of characters in it\n",
    "print(f'Length of text: {len(text)} characters')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "2de37e44",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'First Citizen:\\nBefore we proceed any further, hear me speak.\\n\\nAll:\\nSpeak, speak.\\n\\nFirst Citizen:\\nYou are all resolved rather to die than to famish?\\n\\nAll:\\nResolved. resolved.\\n\\nFirst Citizen:\\nFirst, you know Caius Marcius is chief enemy to the people.\\n'"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "text[:250]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "aa2a78e4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "65 unique characters\n"
     ]
    }
   ],
   "source": [
    "# The unique characters in the file\n",
    "vocab = sorted(set(text))  # List of characters\n",
    "print(f'{len(vocab)} unique characters')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b88446ba",
   "metadata": {},
   "source": [
    "### Process the text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "id": "1bf4dc95",
   "metadata": {},
   "outputs": [],
   "source": [
    "ids_from_chars = tf.keras.layers.StringLookup(\n",
    "    vocabulary=vocab, mask_token=None)  # This returns a tokenizer(character level-as our vocab is just characters!)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ae0c358b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# char_from_ids object is used to convert ids to characters\n",
    "# Notice that we gave it the same reference vocab as we gave to ids_from_chars object\n",
    "# This is because we want the UNK token to be set the same way\n",
    "chars_from_ids = tf.keras.layers.StringLookup(\n",
    "    vocabulary=ids_from_chars.get_vocabulary(), invert=True, mask_token=None)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "id": "b467d84c",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<tf.RaggedTensor [[b'a', b'b', b'c', b'd', b'e', b'f', b'g'], [b'x', b'y', b'z']]>"
      ]
     },
     "execution_count": 69,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "example_texts = ['abcdefg', 'xyz']\n",
    "\n",
    "chars = tf.strings.unicode_split(example_texts, input_encoding='UTF-8')\n",
    "chars"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "8097905d",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<tf.RaggedTensor [[40, 41, 42, 43, 44, 45, 46], [63, 64, 65]]>"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "ids = ids_from_chars(chars)  # Generating numeric sequence\n",
    "ids"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "id": "eeb659a0",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<tf.RaggedTensor [[b'a', b'b', b'c', b'd', b'e', b'f', b'g'], [b'x', b'y', b'z']]>"
      ]
     },
     "execution_count": 41,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "chars = chars_from_ids(ids)\n",
    "chars"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "id": "7fbeb524",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([b'abcdefg', b'xyz'], dtype=object)"
      ]
     },
     "execution_count": 45,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# tf.strings.reduce_join to join the characters back into strings. \n",
    "tf.strings.reduce_join(chars, axis=-1).numpy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "id": "7083e8d0",
   "metadata": {},
   "outputs": [],
   "source": [
    "def text_from_ids(ids):\n",
    "  return tf.strings.reduce_join(chars_from_ids(ids), axis=-1)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "98f8703c",
   "metadata": {},
   "source": [
    "### Creating training and target examples"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "id": "2e2e2694",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<tf.Tensor: shape=(1115394,), dtype=int64, numpy=array([19, 48, 57, ..., 46,  9,  1], dtype=int64)>"
      ]
     },
     "execution_count": 47,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "all_ids = ids_from_chars(tf.strings.unicode_split(text, 'UTF-8'))\n",
    "all_ids"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "id": "5e5ce2a4",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<TensorSliceDataset element_spec=TensorSpec(shape=(), dtype=tf.int64, name=None)>"
      ]
     },
     "execution_count": 49,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "ids_dataset = tf.data.Dataset.from_tensor_slices(all_ids)\n",
    "ids_dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "id": "3f026896",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "F\n",
      "i\n",
      "r\n",
      "s\n",
      "t\n"
     ]
    }
   ],
   "source": [
    "for ids in ids_dataset.take(5):\n",
    "    print(chars_from_ids(ids).numpy().decode('utf-8'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "id": "1fc05456",
   "metadata": {},
   "outputs": [],
   "source": [
    "seq_length = 100"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "id": "7927fcbd",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tf.Tensor(\n",
      "[b'F' b'i' b'r' b's' b't' b' ' b'C' b'i' b't' b'i' b'z' b'e' b'n' b':'\n",
      " b'\\n' b'B' b'e' b'f' b'o' b'r' b'e' b' ' b'w' b'e' b' ' b'p' b'r' b'o'\n",
      " b'c' b'e' b'e' b'd' b' ' b'a' b'n' b'y' b' ' b'f' b'u' b'r' b't' b'h'\n",
      " b'e' b'r' b',' b' ' b'h' b'e' b'a' b'r' b' ' b'm' b'e' b' ' b's' b'p'\n",
      " b'e' b'a' b'k' b'.' b'\\n' b'\\n' b'A' b'l' b'l' b':' b'\\n' b'S' b'p' b'e'\n",
      " b'a' b'k' b',' b' ' b's' b'p' b'e' b'a' b'k' b'.' b'\\n' b'\\n' b'F' b'i'\n",
      " b'r' b's' b't' b' ' b'C' b'i' b't' b'i' b'z' b'e' b'n' b':' b'\\n' b'Y'\n",
      " b'o' b'u' b' '], shape=(101,), dtype=string)\n"
     ]
    }
   ],
   "source": [
    "sequences = ids_dataset.batch(seq_length+1, drop_remainder=True)\n",
    "\n",
    "for seq in sequences.take(1):\n",
    "  print(chars_from_ids(seq))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "id": "04b99708",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "b'First Citizen:\\nBefore we proceed any further, hear me speak.\\n\\nAll:\\nSpeak, speak.\\n\\nFirst Citizen:\\nYou '\n",
      "b'are all resolved rather to die than to famish?\\n\\nAll:\\nResolved. resolved.\\n\\nFirst Citizen:\\nFirst, you k'\n",
      "b\"now Caius Marcius is chief enemy to the people.\\n\\nAll:\\nWe know't, we know't.\\n\\nFirst Citizen:\\nLet us ki\"\n",
      "b\"ll him, and we'll have corn at our own price.\\nIs't a verdict?\\n\\nAll:\\nNo more talking on't; let it be d\"\n",
      "b'one: away, away!\\n\\nSecond Citizen:\\nOne word, good citizens.\\n\\nFirst Citizen:\\nWe are accounted poor citi'\n"
     ]
    }
   ],
   "source": [
    "for seq in sequences.take(5):\n",
    "  print(text_from_ids(seq).numpy())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "id": "84699fa8",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Takes in a string, returns input and target texts (We'll map this func. to our dataset object!)\n",
    "def split_input_target(sequence):\n",
    "    input_text = sequence[:-1]\n",
    "    target_text = sequence[1:]\n",
    "    return input_text, target_text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "id": "61c07afe",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(['T', 'e', 'n', 's', 'o', 'r', 'f', 'l', 'o'],\n",
       " ['e', 'n', 's', 'o', 'r', 'f', 'l', 'o', 'w'])"
      ]
     },
     "execution_count": 64,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "split_input_target(list(\"Tensorflow\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "id": "c44a41db",
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset = sequences.map(split_input_target)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "id": "678092e9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Input : b'First Citizen:\\nBefore we proceed any further, hear me speak.\\n\\nAll:\\nSpeak, speak.\\n\\nFirst Citizen:\\nYou'\n",
      "Target: b'irst Citizen:\\nBefore we proceed any further, hear me speak.\\n\\nAll:\\nSpeak, speak.\\n\\nFirst Citizen:\\nYou '\n"
     ]
    }
   ],
   "source": [
    "for input_example, target_example in dataset.take(1):\n",
    "    print(\"Input :\", text_from_ids(input_example).numpy())\n",
    "    print(\"Target:\", text_from_ids(target_example).numpy())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "913a77c5",
   "metadata": {},
   "source": [
    "### Creating training batches"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "id": "9e858bf3",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<PrefetchDataset element_spec=(TensorSpec(shape=(64, 100), dtype=tf.int64, name=None), TensorSpec(shape=(64, 100), dtype=tf.int64, name=None))>"
      ]
     },
     "execution_count": 67,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Batch size\n",
    "BATCH_SIZE = 64\n",
    "\n",
    "# Buffer size to shuffle the dataset\n",
    "# (TF data is designed to work with possibly infinite sequences,\n",
    "# so it doesn't attempt to shuffle the entire sequence in memory. Instead,\n",
    "# it maintains a buffer in which it shuffles elements).\n",
    "BUFFER_SIZE = 10000\n",
    "\n",
    "dataset = (\n",
    "    dataset\n",
    "    .shuffle(BUFFER_SIZE)\n",
    "    .batch(BATCH_SIZE, drop_remainder=True)\n",
    "    .prefetch(tf.data.experimental.AUTOTUNE))\n",
    "\n",
    "dataset"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1805ef84",
   "metadata": {},
   "source": [
    "### Build The Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "id": "b1718537",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Length of the vocabulary in StringLookup Layer\n",
    "vocab_size = len(ids_from_chars.get_vocabulary())\n",
    "\n",
    "# The embedding dimension\n",
    "embedding_dim = 256\n",
    "\n",
    "# Number of RNN units\n",
    "rnn_units = 1024"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "id": "dc9574ec",
   "metadata": {},
   "outputs": [],
   "source": [
    "class MyModel(tf.keras.Model):\n",
    "  def __init__(self, vocab_size, embedding_dim, rnn_units):\n",
    "    super().__init__(self)\n",
    "    self.embedding = tf.keras.layers.Embedding(vocab_size, embedding_dim)\n",
    "    self.gru = tf.keras.layers.GRU(rnn_units,\n",
    "                                   return_sequences=True, # we'll get output y at each timestamp through the Dense layer\n",
    "                                   return_state=True)  # Every gru layer passes hidden activations to next layer\n",
    "    self.dense = tf.keras.layers.Dense(vocab_size)\n",
    "\n",
    "  def call(self, inputs, states=None, return_state=False, training=False):\n",
    "    x = inputs\n",
    "    x = self.embedding(x, training=training)\n",
    "    if states is None:\n",
    "      states = self.gru.get_initial_state(x)\n",
    "    x, states = self.gru(x, initial_state=states, training=training)\n",
    "    x = self.dense(x, training=training)\n",
    "\n",
    "    if return_state:\n",
    "      return x, states\n",
    "    else:\n",
    "      return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "id": "55e8626a",
   "metadata": {},
   "outputs": [],
   "source": [
    "model = MyModel(\n",
    "    vocab_size=vocab_size,\n",
    "    embedding_dim=embedding_dim,\n",
    "    rnn_units=rnn_units)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ddc5ce78",
   "metadata": {},
   "source": [
    "### Trying the Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "id": "bca81ff5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(64, 100, 66) # (batch_size, sequence_length, vocab_size)\n"
     ]
    }
   ],
   "source": [
    "for input_example_batch, target_example_batch in dataset.take(1):\n",
    "    example_batch_predictions = model(input_example_batch)\n",
    "    print(example_batch_predictions.shape, \"# (batch_size, sequence_length, vocab_size)\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "id": "e6eed0dd",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"my_model\"\n",
      "_________________________________________________________________\n",
      " Layer (type)                Output Shape              Param #   \n",
      "=================================================================\n",
      " embedding (Embedding)       multiple                  16896     \n",
      "                                                                 \n",
      " gru (GRU)                   multiple                  3938304   \n",
      "                                                                 \n",
      " dense (Dense)               multiple                  67650     \n",
      "                                                                 \n",
      "=================================================================\n",
      "Total params: 4,022,850\n",
      "Trainable params: 4,022,850\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "model.summary()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "37e69962",
   "metadata": {},
   "source": [
    "To get actual predictions from the model you need to sample from the output distribution, to get actual character indices. This distribution is defined by the logits over the character vocabulary.\n",
    "\n",
    "Note: It is important to _sample_ from this distribution as taking the _argmax_ of the distribution can easily get the model stuck in a loop.\n",
    "\n",
    "Try it for the first example in the batch:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "id": "de5460d0",
   "metadata": {},
   "outputs": [],
   "source": [
    "sampled_indices = tf.random.categorical(example_batch_predictions[0], num_samples=1)\n",
    "sampled_indices = tf.squeeze(sampled_indices, axis=-1).numpy()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2365e0cd",
   "metadata": {},
   "source": [
    "This gives us, at each timestep, a prediction of the next character index:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "id": "cb967f65",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([31,  2, 34, 44, 62, 47, 49, 38, 19, 33,  1, 40, 65, 18, 28, 59, 38,\n",
       "       22, 35, 50,  9, 47,  1, 46, 42, 64, 53,  1, 25, 16, 26, 14, 27, 37,\n",
       "       52, 55, 25, 62, 38, 22, 23, 43, 16,  4, 46, 56,  1, 32, 24, 18,  6,\n",
       "       55, 41, 11, 44,  3,  4, 19, 14, 18,  7,  0, 34, 48, 36, 40, 39, 48,\n",
       "       36, 10,  7, 16, 25, 22, 35, 52, 39, 21, 55,  4, 25, 42, 37, 46, 51,\n",
       "       62, 12,  0, 60, 42, 29, 29, 15, 30,  1, 12, 11, 23, 55, 34],\n",
       "      dtype=int64)"
      ]
     },
     "execution_count": 76,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sampled_indices"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "id": "abff4315",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Input:\n",
      " b\"dness! Make not impossible\\nThat which but seems unlike: 'tis not impossible\\nBut one, the wicked'st c\"\n",
      "\n",
      "Next Char Predictions:\n",
      " b\"R UewhjYFT\\nazEOtYIVk.h\\ngcyn\\nLCMANXmpLwYIJdC$gq\\nSKE'pb:e!$FAE,[UNK]UiWaZiW3,CLIVmZHp$LcXglw;[UNK]ucPPBQ\\n;:JpU\"\n"
     ]
    }
   ],
   "source": [
    "print(\"Input:\\n\", text_from_ids(input_example_batch[0]).numpy())\n",
    "print()\n",
    "print(\"Next Char Predictions:\\n\", text_from_ids(sampled_indices).numpy())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1a33f530",
   "metadata": {},
   "source": [
    "### Train the Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "id": "a5f81074",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Since the model returns logits, make the from_logits flag set to true.\n",
    "loss = tf.losses.SparseCategoricalCrossentropy(from_logits=True) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 80,
   "id": "3d5e521a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Prediction shape:  (64, 100, 66)  # (batch_size, sequence_length, vocab_size)\n",
      "Mean loss:         tf.Tensor(4.1894026, shape=(), dtype=float32)\n"
     ]
    }
   ],
   "source": [
    "example_batch_mean_loss = loss(target_example_batch, example_batch_predictions)\n",
    "print(\"Prediction shape: \", example_batch_predictions.shape, \" # (batch_size, sequence_length, vocab_size)\")\n",
    "print(\"Mean loss:        \", example_batch_mean_loss)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "746ca993",
   "metadata": {},
   "source": [
    "A newly initialized model shouldn't be too sure of itself, the output logits should all have similar magnitudes. To confirm this you can check that the exponential of the mean loss is approximately equal to the vocabulary size. A much higher loss means the model is sure of its wrong answers, and is badly initialized:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 81,
   "id": "eeac413b",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "65.98336"
      ]
     },
     "execution_count": 81,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tf.exp(example_batch_mean_loss).numpy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 82,
   "id": "100d352f",
   "metadata": {},
   "outputs": [],
   "source": [
    "model.compile(optimizer='adam', loss=loss)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "22c9e1f2",
   "metadata": {},
   "source": [
    "### Creating Checkpoints"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 83,
   "id": "7b092a8e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Directory where the checkpoints will be saved\n",
    "checkpoint_dir = './training_checkpoints'\n",
    "# Name of the checkpoint files\n",
    "checkpoint_prefix = os.path.join(checkpoint_dir, \"ckpt_{epoch}\")\n",
    "\n",
    "checkpoint_callback = tf.keras.callbacks.ModelCheckpoint(\n",
    "    filepath=checkpoint_prefix,\n",
    "    save_weights_only=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "514bec72",
   "metadata": {},
   "source": [
    "### Execute the training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 84,
   "id": "9271d43d",
   "metadata": {},
   "outputs": [],
   "source": [
    "EPOCHS = 5"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 85,
   "id": "f79f107f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/5\n",
      "172/172 [==============================] - 608s 4s/step - loss: 2.7193\n",
      "Epoch 2/5\n",
      "172/172 [==============================] - 14978s 88s/step - loss: 1.9818\n",
      "Epoch 3/5\n",
      "172/172 [==============================] - 468s 3s/step - loss: 1.7016\n",
      "Epoch 4/5\n",
      "172/172 [==============================] - 457s 3s/step - loss: 1.5420\n",
      "Epoch 5/5\n",
      "172/172 [==============================] - 460s 3s/step - loss: 1.4441\n"
     ]
    }
   ],
   "source": [
    "history = model.fit(dataset, epochs=EPOCHS, callbacks=[checkpoint_callback])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5d6458e8",
   "metadata": {},
   "source": [
    "### Generate text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 86,
   "id": "a3f8239d",
   "metadata": {},
   "outputs": [],
   "source": [
    "class OneStep(tf.keras.Model):\n",
    "  def __init__(self, model, chars_from_ids, ids_from_chars, temperature=1.0):\n",
    "    super().__init__()\n",
    "    self.temperature = temperature\n",
    "    self.model = model\n",
    "    self.chars_from_ids = chars_from_ids\n",
    "    self.ids_from_chars = ids_from_chars\n",
    "\n",
    "    # Create a mask to prevent \"[UNK]\" from being generated.\n",
    "    skip_ids = self.ids_from_chars(['[UNK]'])[:, None]\n",
    "    sparse_mask = tf.SparseTensor(\n",
    "        # Put a -inf at each bad index.\n",
    "        values=[-float('inf')]*len(skip_ids),\n",
    "        indices=skip_ids,\n",
    "        # Match the shape to the vocabulary\n",
    "        dense_shape=[len(ids_from_chars.get_vocabulary())])\n",
    "    self.prediction_mask = tf.sparse.to_dense(sparse_mask)\n",
    "\n",
    "  @tf.function\n",
    "  def generate_one_step(self, inputs, states=None):\n",
    "    # Convert strings to token IDs.\n",
    "    input_chars = tf.strings.unicode_split(inputs, 'UTF-8')\n",
    "    input_ids = self.ids_from_chars(input_chars).to_tensor()\n",
    "\n",
    "    # Run the model.\n",
    "    # predicted_logits.shape is [batch, char, next_char_logits]\n",
    "    predicted_logits, states = self.model(inputs=input_ids, states=states,\n",
    "                                          return_state=True)\n",
    "    # Only use the last prediction.\n",
    "    predicted_logits = predicted_logits[:, -1, :]\n",
    "    predicted_logits = predicted_logits/self.temperature\n",
    "    # Apply the prediction mask: prevent \"[UNK]\" from being generated.\n",
    "    predicted_logits = predicted_logits + self.prediction_mask\n",
    "\n",
    "    # Sample the output logits to generate token IDs.\n",
    "    predicted_ids = tf.random.categorical(predicted_logits, num_samples=1)\n",
    "    predicted_ids = tf.squeeze(predicted_ids, axis=-1)\n",
    "\n",
    "    # Convert from token ids to characters\n",
    "    predicted_chars = self.chars_from_ids(predicted_ids)\n",
    "\n",
    "    # Return the characters and model state.\n",
    "    return predicted_chars, states"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 87,
   "id": "1b0412c0",
   "metadata": {},
   "outputs": [],
   "source": [
    "one_step_model = OneStep(model, chars_from_ids, ids_from_chars)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e064adf5",
   "metadata": {},
   "source": [
    "Run it in a loop to generate some text. Looking at the generated text, you'll see the model knows when to capitalize, make paragraphs and imitates a Shakespeare-like writing vocabulary. With the small number of training epochs, it has not yet learned to form coherent sentences."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 88,
   "id": "a01b7056",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ROMEO:\n",
      "-this is a preaser tomple shall prove me, ord my grie writnes;\n",
      "Come, in thire and myself:\n",
      "I am again mines;\n",
      "And then know father\n",
      "Or doathance I'll bear this, tear'st.\n",
      "\n",
      "AmmI:\n",
      "Sir, I worr they, thereword won;\n",
      "Harry you said---Op do quickle your devorm,\n",
      "And may I can past to them,\n",
      "If not in mine dow, it zadisht\n",
      "that thou heart your hands I pretty,\n",
      "'Tis proud, 'Dile no your it on the queen!\n",
      "\n",
      "POMP:\n",
      "A since to your seaved's house in the dial 'goo?\n",
      "\n",
      "POLIXENES:\n",
      "I may came feroul of thy nearts of morture\n",
      "But head as favour tailt.\n",
      "\n",
      "NORTHUMBERLAND:\n",
      "Then, madam, oney.\n",
      "\n",
      "FRIAR LAURENCE:\n",
      "Eecle him distance\n",
      "To citize-sinking shile; chalge thy dead.\n",
      "\n",
      "ISABELLA:\n",
      "Hat scand it sweet rage the towards.\n",
      "\n",
      "PETER:\n",
      "I printers and delay'd.' Hanton: think to more out.\n",
      "Must is every blood want;\n",
      "What I cannot him, in hotest part his art to wink.\n",
      "\n",
      "KING EDWARD II:\n",
      "Which he shall still fetch forfurred;\n",
      "His a foush tormers from the day,\n",
      "You are un and holse victor.\n",
      "\n",
      "KATHARINA:\n",
      "I'll to you the duke of king?\n",
      "\n",
      "BULINTE:\n",
      "A h \n",
      "\n",
      "________________________________________________________________________________\n",
      "\n",
      "Run time: 4.0562522411346436\n"
     ]
    }
   ],
   "source": [
    "start = time.time()\n",
    "states = None\n",
    "next_char = tf.constant(['ROMEO:'])\n",
    "result = [next_char]\n",
    "\n",
    "for n in range(1000):\n",
    "  next_char, states = one_step_model.generate_one_step(next_char, states=states)\n",
    "  result.append(next_char)\n",
    "\n",
    "result = tf.strings.join(result)\n",
    "end = time.time()\n",
    "print(result[0].numpy().decode('utf-8'), '\\n\\n' + '_'*80)\n",
    "print('\\nRun time:', end - start)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5ab64587",
   "metadata": {},
   "source": [
    "If you want the model to generate text faster the easiest thing you can do is batch the text generation. In the example below the model generates 5 outputs in about the same time it took to generate 1 above. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 90,
   "id": "484ea10b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tf.Tensor(\n",
      "[b\"ROMEO:\\nChallow, play. What lay in the city Coriolanumina, 'Tis dispade\\nAnd tell our darnenss meet of dienct?\\n\\nRAMELLI:\\nBut you ara wonder's plage, 'tis your mistress'\\nIs there not to blize thank the evilitainst\\nA thee is grince: he's and rehal falls,\\nThat that 'twixt the prince: say'd.\\n\\nLEONTES:\\nI'll foll what hath done esoce\\nHis lift: but rest much lords; and to make their\\ntreators arm the bloodions: and thou entrems\\nshoublest I depright I on you?\\nTirra, that good sight, you gract is here. Come, sincerous,\\nWould hath me save a wifou of Here order?\\nPentlemity on winten and the world:\\nThis, I darken no orry tongue in\\nthis and cell this hands, that's name, my ond mores in that tifle,\\nIn it to misery mockard.\\n\\nMAPILLIUS:\\nYea, not so ediliance she down thyself?\\nThat thou to a night?\\n\\nLADY GREY:\\nWell you, but granty serve it, and do revile.\\n\\nVOLUMNIA:\\nLond Henry heremis;\\nOver what thou art not sumpled mine.\\nShe was, to her tria sorry Calible,\\nLet him stamb'd'st that I defendam?\\n\\nProvost:\\nCome, go\"\n",
      " b\"ROMEO:\\nGod yet, I be dured, Pray: gensever, her not to make\\nThat I shall not truly!' give me Alla's hand:\\nThe ajus or this hour of thought I hea,\\nO hep no out audiness: what I daint that tiger;\\nIf shall be redect by cut onftractonarisy vextle,\\nAnd to crought can on good hatf thou art,\\nWith along on him, pasted\\nYoung most proclipy your hands.\\n\\nTRANIO:\\nIt shall not give hence her shadow.\\n\\nESCALUS:\\nUn my little hery, live;\\nlive, marry, I ten tail our hast.\\nGod why's none shirst and with his heart,\\nWith true cuntilut of the fairht I rope\\nYour have here aiviods have unsulder acond;\\nDow I will wanton I lear made him.\\n\\nCORIOLANUS:\\nI sake thee have delivertied I sin!\\n\\nNORTHUMBERLAND:\\nThat cunnin'st to live his comination.\\n\\nCORIOLANUS:\\nAnd you will renemity.\\nFear it the enjiry's hadry confestable\\nThau thee the head? Of,\\nWhich, I know 'twas draw you as the interta's oath, if advere\\nin the chy of trit shamph me, you\\nwill prent to bed his brother?\\n\\nBEONENEL:\\nA gove a halphos? standellow when they: healt\"\n",
      " b\"ROMEO:\\nBut there thou art thou: would to\\nWalour and a glorital-mistes,\\nAnd here crown! do goner, and to please your lords.\\n\\nPOMPIY:\\nExferce me him and myself as as I love humal.\\nHow now! what's the Volsce as younge dissown, and think\\nI have marry else vervity:\\nif our my still be thy beator\\nwith him, the raisence contram,\\nWhen I pare dovine that\\nA painty shepch to suffice; and I beoked the\\nFirence, pains apoved some point: I have does a good and Warwick, as I have crozn bed remeal.\\nNever sted his heart understand very friend.\\n\\nLord:\\nI to you it! Hercition, me yet,\\nIn shadow in ambasse Intural confining;\\nThat is summer day; and Judief the countert's brums;\\nYour kin'd up you to ply: you are\\nBut fortince lighty your hander.\\nUnfict thou liest;\\nHere consoup in big to this hand; for whom of eyes?\\n\\nSEBASTIAN:\\nTown you, sir, his off heaven from is.\\n\\nCAMILLO:\\nI may come to more, to marry, I will not progle first\\nThe town? if were by swain not.\\n\\nhere yot nave,\\nWhow makes are did, that could not burged \"\n",
      " b\"ROMEO:\\nWautht reboused this say: O my lord;\\nI the inl upon to\\nTeld I play old many ten, in mishrews, it shall bemold their vimbore:\\nAnd you shall wretch him have proind him luties.\\nWell not, in my lord. Or else so believe, we bears a prayert.\\n\\nFirst Kate:\\n'Tis that ' this now as a truamberts,\\nWithin this whrint the vowards of his bright:\\nwith a dead, makes the ground: gentle say Bostistal prince:\\nInouth reporting ere to-darous death'd\\nand taye of vercain thunger should and mpen.\\nBuck our parting speechetue--art thy answer way\\nHis day in another: I pray.\\n\\nMARIALA:\\nThat year out of mought.\\n\\nLord:\\n\\nServant:\\nAlocks, as it that I sulrence:\\nAnd betart it is'e to bldoke my capernt, whose hader do patom,\\nAnd how that bediness him request my\\nyoung nobled?\\n\\nSICINIUS:\\nMadabrill as a lamenty, gave me, next to the crown's\\nThan day he hath done to trust me it now:\\nI'll not from me,--\\nDet it in the uncon the gods that I'll in grave!\\nYou think; he hath not may?\\n\\nHASTINGS:\\nFe'll say: griving redol to quile a\"\n",
      " b\"ROMEO:\\nThen leave you a point of moch.\\n\\nHOMTENS:\\nIs gencleman: York, Procears Than Poor, godd:\\nWhich she better homethip wounds?\\n\\nAUTOLYCUS:\\nCloud fir my nobar.\\n\\nProvotting:\\nYet, as he should doma ne'er ambarlion.\\n\\nHASTINGS:\\nGod so we private more sweet speak, mighty nobles\\nThat God 'veirs thou let that take it is more.\\n\\nPETRUCHIO:\\nAnd that play subject things\\nProud hence to yet sounds, te'll say in other\\ntill than his hand.\\n\\nDUKE VINCENTIO:\\nThat's this counteran: if she may have tongue words his grief,\\nIn the dorn to quie and dolice,\\nOut on night, to have I speak no brimite\\nHer hopted, looks preduble\\nTo u bewilk that doint against he bit\\nsone my hand of her bringhing seems,--lady?\\nYou, sir, 'tis a law, my arm.\\n\\nCUSTILYO:\\nLove not, Anstive may suppute I am aquolly heand,\\nAbout this fright him and and have dead.\\nHee, For the master, a cateful head\\nAuthout with the princes to late:\\nThe right of your embliate,\\nAnd you gring in thy treather of the sileness.\\n\\nFRIAR JUEEN:\\nTrail and quailial. Gove\"], shape=(5,), dtype=string) \n",
      "\n",
      "________________________________________________________________________________\n",
      "\n",
      "Run time: 3.306943893432617\n"
     ]
    }
   ],
   "source": [
    "start = time.time()\n",
    "states = None\n",
    "next_char = tf.constant(['ROMEO:', 'ROMEO:', 'ROMEO:', 'ROMEO:', 'ROMEO:'])\n",
    "result = [next_char]\n",
    "\n",
    "for n in range(1000):\n",
    "  next_char, states = one_step_model.generate_one_step(next_char, states=states)\n",
    "  result.append(next_char)\n",
    "\n",
    "result = tf.strings.join(result)\n",
    "end = time.time()\n",
    "print(result, '\\n\\n' + '_'*80)\n",
    "print('\\nRun time:', end - start)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "625cd99c",
   "metadata": {},
   "source": [
    "### Export the generator"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8140bcbb",
   "metadata": {},
   "outputs": [],
   "source": [
    "tf.saved_model.save(one_step_model, 'one_step')\n",
    "one_step_reloaded = tf.saved_model.load('one_step')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "baf795aa",
   "metadata": {},
   "outputs": [],
   "source": [
    "states = None\n",
    "next_char = tf.constant(['ROMEO:'])\n",
    "result = [next_char]\n",
    "\n",
    "for n in range(100):\n",
    "  next_char, states = one_step_reloaded.generate_one_step(next_char, states=states)\n",
    "  result.append(next_char)\n",
    "\n",
    "print(tf.strings.join(result)[0].numpy().decode(\"utf-8\"))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
